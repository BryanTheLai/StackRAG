{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c75db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: c:\\Users\\wbrya\\OneDrive\\Documents\\GitHub\\AI-CFO-FYP\n",
      "Current working directory: c:\\Users\\wbrya\\OneDrive\\Documents\\GitHub\\AI-CFO-FYP\\notebooks\n",
      "\n",
      "--- Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup - Add project root to sys.path and install necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import uuid\n",
    "from pathlib import Path # Use pathlib for easier path handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Ensure necessary libraries are installed (re-run if needed)\n",
    "# !pip install -r ../requirements.txt\n",
    "# !pip install supabase python-dotenv pydantic google-genai openai pymupdf chonkie\n",
    "# !pip install -e . # Install local src package\n",
    "\n",
    "print(\"\\n--- Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b73abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported IngestionPipeline and Supabase client.\n",
      "\n",
      "--- Imports Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import necessary modules and the IngestionPipeline\n",
    "from supabase import create_client, Client\n",
    "from src.pipeline import IngestionPipeline # Import the main pipeline class\n",
    "\n",
    "print(\"Imported IngestionPipeline and Supabase client.\")\n",
    "print(\"\\n--- Imports Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98be7e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPABASE_URL Loaded: Yes\n",
      "SUPABASE_ANON_KEY Loaded: Yes\n",
      "TEST_EMAIL Loaded: Yes\n",
      "TEST_PASSWORD Loaded: Yes\n",
      "Target TEST_USER_UID: 372ec112-6fd7-46a7-bab4-abf7623fb05b\n",
      "\n",
      "Initializing Supabase client...\n",
      "Supabase client initialized.\n",
      "\n",
      "Attempting to sign in user: user@example.com...\n",
      "Sign-in successful for user ID: 372ec112-6fd7-46a7-bab4-abf7623fb05b\n",
      "Logged-in user ID matches target TEST_USER_UID.\n",
      "\n",
      "--- Client Initialization and Authentication Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Environment Variables and Authenticate Test User for Supabase Client\n",
    "load_dotenv() # Load variables from .env file\n",
    "\n",
    "# Get Supabase credentials\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_ANON_KEY\") # Use ANON key\n",
    "\n",
    "# --- Test User Credentials (MUST match the TEST_USER_UID below) ---\n",
    "test_email = os.environ.get(\"TEST_EMAIL\")\n",
    "test_password = os.environ.get(\"TEST_PASSWORD\")\n",
    "# --- Target User UID ---\n",
    "TEST_USER_UID_STR = \"372ec112-6fd7-46a7-bab4-abf7623fb05b\"\n",
    "TEST_USER_UID = uuid.UUID(TEST_USER_UID_STR)\n",
    "\n",
    "print(f\"SUPABASE_URL Loaded: {'Yes' if supabase_url else 'No'}\")\n",
    "print(f\"SUPABASE_ANON_KEY Loaded: {'Yes' if supabase_key else 'No'}\")\n",
    "print(f\"TEST_EMAIL Loaded: {'Yes' if test_email else 'No'}\")\n",
    "print(f\"TEST_PASSWORD Loaded: {'Yes' if test_password else 'No'}\")\n",
    "print(f\"Target TEST_USER_UID: {TEST_USER_UID}\")\n",
    "\n",
    "# Basic validation\n",
    "if not all([supabase_url, supabase_key, test_email, test_password]):\n",
    "    print(\"\\nError: Missing required Supabase credentials or test user info in .env file.\")\n",
    "    supabase_client_authenticated: Client | None = None\n",
    "else:\n",
    "    try:\n",
    "        # Initialize Supabase client\n",
    "        print(\"\\nInitializing Supabase client...\")\n",
    "        temp_client: Client | None = create_client(supabase_url, supabase_key)\n",
    "        print(\"Supabase client initialized.\")\n",
    "\n",
    "        # --- Authenticate the client ---\n",
    "        print(f\"\\nAttempting to sign in user: {test_email}...\")\n",
    "        response = temp_client.auth.sign_in_with_password(\n",
    "            {\"email\": test_email, \"password\": test_password}\n",
    "        )\n",
    "\n",
    "        if response and response.session and response.user:\n",
    "            print(f\"Sign-in successful for user ID: {response.user.id}\")\n",
    "            if str(response.user.id) == TEST_USER_UID_STR:\n",
    "                 print(\"Logged-in user ID matches target TEST_USER_UID.\")\n",
    "                 # Store the authenticated client for the pipeline\n",
    "                 supabase_client_authenticated = temp_client\n",
    "            else:\n",
    "                print(f\"CRITICAL ERROR: Logged-in user ID ({response.user.id}) does NOT match target ({TEST_USER_UID}).\")\n",
    "                supabase_client_authenticated = None\n",
    "        else:\n",
    "            print(\"Sign-in failed. Response:\", response)\n",
    "            supabase_client_authenticated = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError initializing Supabase client or signing in: {e}\")\n",
    "        supabase_client_authenticated = None\n",
    "\n",
    "print(\"\\n--- Client Initialization and Authentication Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494778d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running the Full Ingestion Pipeline ---\n",
      "Processing file: 10k_tesla_3_pages.pdf (Type: pdf)\n",
      "Owned by User ID: 372ec112-6fd7-46a7-bab4-abf7623fb05b\n",
      "Instantiating IngestionPipeline...\n",
      "SupabaseService initialized with provided client.\n",
      "Initializing IngestionPipeline...\n",
      "Initializing Gemini client with API key: AIz...yQ\n",
      "Initialized OpenAI client with model: text-embedding-3-small\n",
      "Initialized Sectioner.\n",
      "Initializing ChunkingService...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wbrya\\OneDrive\\Documents\\GitHub\\AI-CFO-FYP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=2048, min_chars=24\n",
      "Initialized EmbeddingService using model: text-embedding-3-small\n",
      "IngestionPipeline initialized with all services.\n",
      "Pipeline instantiated.\n",
      "\n",
      "--- Starting Ingestion Pipeline for: 10k_tesla_3_pages.pdf (User: 372ec112-6fd7-46a7-bab4-abf7623fb05b) ---\n",
      "\n",
      "Step 1: Parsing PDF to Markdown...\n",
      "PDF has 3 pages\n",
      "Rendering page 1/3\n",
      "Rendering page 2/3\n",
      "Rendering page 3/3\n",
      "Starting page annotation with max 3 concurrent workers...\n",
      "Page 1: Annotating (Attempt 1/6)\n",
      "Page 2: Annotating (Attempt 1/6)\n",
      "Page 3: Annotating (Attempt 1/6)\n",
      "Page 2: Annotation successful.\n",
      "Page 1: Annotation successful.\n",
      "Page 3: Annotation successful.\n",
      "Parsing successful (3 pages). Markdown length: 12684\n",
      "\n",
      "Step 2: Extracting Document Metadata...\n",
      "Sending text snippet to LLM for structured metadata extractionâ€¦\n",
      "Structured metadata extraction attempted.\n",
      "Metadata extraction successful.\n",
      "  Extracted Type: Annual Report\n",
      "  Extracted Company: Tesla, Inc.\n",
      "\n",
      "Step 3: Uploading Original PDF (using temp ID for path: 8faec042-8dcd-43aa-8131-0e739d729d4d)...\n",
      "Attempting to upload PDF to storage path: 372ec112-6fd7-46a7-bab4-abf7623fb05b/8faec042-8dcd-43aa-8131-0e739d729d4d/10k_tesla_3_pages.pdf\n",
      "Supabase storage upload response for 372ec112-6fd7-46a7-bab4-abf7623fb05b/8faec042-8dcd-43aa-8131-0e739d729d4d/10k_tesla_3_pages.pdf\n",
      "PDF successfully uploaded to: 372ec112-6fd7-46a7-bab4-abf7623fb05b/8faec042-8dcd-43aa-8131-0e739d729d4d/10k_tesla_3_pages.pdf\n",
      "Original PDF uploaded successfully to: 372ec112-6fd7-46a7-bab4-abf7623fb05b/8faec042-8dcd-43aa-8131-0e739d729d4d/10k_tesla_3_pages.pdf\n",
      "\n",
      "Step 4: Saving Document Record to Database...\n",
      "Saving document record for: 10k_tesla_3_pages.pdf (User: 372ec112-6fd7-46a7-bab4-abf7623fb05b)\n",
      "Document record saved successfully. Document ID: 70534719-0dd7-4a6e-914d-07f767b71f4b\n",
      "Document record saved successfully. Document ID: 70534719-0dd7-4a6e-914d-07f767b71f4b\n",
      "\n",
      "Step 5: Sectioning Markdown Content...\n",
      "Sectioning markdown content (156 lines)...\n",
      "Finalized section 0: 'Document Start' (Pages: [1])\n",
      "Finalized section 1: 'UNITED STATES' (Pages: [1])\n",
      "Finalized section 2: 'SECURITIES AND EXCHANGE COMMISSION' (Pages: [1])\n",
      "Finalized section 3: 'Washington, D.C. 20549' (Pages: [1])\n",
      "Finalized section 4: 'FORM 10-K' (Pages: [1, 2])\n",
      "Finalized section 5: 'TESLA, INC.' (Pages: [2])\n",
      "Finalized section 6: 'ANNUAL REPORT ON FORM 10-K FOR THE YEAR ENDED DECEMBER 31, 2021' (Pages: [2])\n",
      "Finalized section 7: 'INDEX' (Pages: [2, 3])\n",
      "Finalized section 8: 'Note 4 â€“ Goodwill and Intangible Assets' (Pages: [3])\n",
      "Finalized last section 9: 'Note 5 â€“ Fair Value of Financial Instruments' (Pages: [3])\n",
      "Sectioning complete. Created 10 sections.\n",
      "\n",
      "Step 6: Saving Sections to Database...\n",
      "Saving batch of 10 section records...\n",
      "Batch of 10 sections saved successfully.\n",
      "Sections saved successfully (10 sections).\n",
      "\n",
      "Step 7: Chunking Sections...\n",
      "Chunking 10 sections...\n",
      "Chunking 'Document Start'...\n",
      "'Document Start' â†’ 1 chunks\n",
      "Chunking 'UNITED STATES'...\n",
      "'UNITED STATES' â†’ 1 chunks\n",
      "Chunking 'SECURITIES AND EXCHANGE COMMISSION'...\n",
      "'SECURITIES AND EXCHANGE COMMISSION' â†’ 1 chunks\n",
      "Chunking 'Washington, D.C. 20549'...\n",
      "'Washington, D.C. 20549' â†’ 1 chunks\n",
      "Chunking 'FORM 10-K'...\n",
      "'FORM 10-K' â†’ 1 chunks\n",
      "Chunking 'TESLA, INC.'...\n",
      "'TESLA, INC.' â†’ 1 chunks\n",
      "Chunking 'ANNUAL REPORT ON FORM 10-K FOR THE YEAR ENDED DECEMBER 31, 2021'...\n",
      "'ANNUAL REPORT ON FORM 10-K FOR THE YEAR ENDED DECEMBER 31, 2021' â†’ 1 chunks\n",
      "Chunking 'INDEX'...\n",
      "'INDEX' â†’ 1 chunks\n",
      "Chunking 'Note 4 â€“ Goodwill and Intangible Assets'...\n",
      "'Note 4 â€“ Goodwill and Intangible Assets' â†’ 4 chunks\n",
      "Chunking 'Note 5 â€“ Fair Value of Financial Instruments'...\n",
      "'Note 5 â€“ Fair Value of Financial Instruments' â†’ 2 chunks\n",
      "Done: 14 chunks total\n",
      "\n",
      "Step 8: Generating Embeddings...\n",
      "Generating embeddings for 14 chunks using model: text-embedding-3-small...\n",
      "Prepared 14 augmented texts for embedding.\n",
      "Successfully added embeddings to 14 chunks.\n",
      "Embeddings generated successfully.\n",
      "\n",
      "Step 9: Saving Chunks with Embeddings to Database...\n",
      "Saving batch of 14 chunk records...\n",
      "Attempting bulk insert of 14 chunks...\n",
      "Batch of 14 chunks likely saved successfully.\n",
      "Chunks saved successfully.\n",
      "\n",
      "Step 10: Updating Document Status to Completed...\n",
      "Updating status for document 70534719-0dd7-4a6e-914d-07f767b71f4b to 'completed'\n",
      "Document status updated successfully.\n",
      "\n",
      "--- Ingestion Pipeline Successfully Completed for 10k_tesla_3_pages.pdf in 19.67 seconds ---\n",
      "\n",
      "--- Pipeline Run Result ---\n",
      "Success: True\n",
      "Message: Document processed and ingested successfully.\n",
      "Document ID: 70534719-0dd7-4a6e-914d-07f767b71f4b\n",
      "Chunks Generated: 14\n",
      "\n",
      "--- Pipeline Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Test File Path and Run the Ingestion Pipeline\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the PDF file you want to process, relative to the project root\n",
    "# Make sure this file exists!\n",
    "from src.storage.SupabaseService import SupabaseService\n",
    "\n",
    "\n",
    "PDF_RELATIVE_PATH = \"data/source_pdfs/10k_tesla_3_pages.pdf\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Variable to store the result of the pipeline run\n",
    "pipeline_run_result = None\n",
    "final_document_id = None\n",
    "\n",
    "# Proceed only if we have an authenticated Supabase client\n",
    "if supabase_client_authenticated:\n",
    "    print(\"\\n--- Running the Full Ingestion Pipeline ---\")\n",
    "\n",
    "    # Construct the full path to the PDF file\n",
    "    pdf_full_path = Path(project_root) / PDF_RELATIVE_PATH\n",
    "    original_filename = pdf_full_path.name\n",
    "    doc_type = pdf_full_path.suffix[1:].lower() # Get 'pdf' from '.pdf'\n",
    "\n",
    "    if not pdf_full_path.is_file():\n",
    "        print(f\"Error: Test PDF file not found at {pdf_full_path}\")\n",
    "    else:\n",
    "        print(f\"Processing file: {original_filename} (Type: {doc_type})\")\n",
    "        print(f\"Owned by User ID: {TEST_USER_UID}\")\n",
    "\n",
    "        try:\n",
    "            # Instantiate the IngestionPipeline\n",
    "            # We can pass the authenticated client to the SupabaseService instance it creates\n",
    "            # (or modify pipeline __init__ to accept and pass clients)\n",
    "            # For now, we rely on SupabaseService creating its own client,\n",
    "            # which will pick up the auth context set by sign_in_with_password\n",
    "            # on the shared supabase_py instance state (this is how supabase-py often works).\n",
    "            # A more robust approach would be explicit client passing.\n",
    "            print(\"Instantiating IngestionPipeline...\")\n",
    "            pipeline = IngestionPipeline(\n",
    "                 # Explicitly pass the authenticated client to SupabaseService if preferred:\n",
    "                 supabase_service=SupabaseService(supabase_client=supabase_client_authenticated)\n",
    "                 # Otherwise, rely on the default __init__ behavior.\n",
    "            )\n",
    "            print(\"Pipeline instantiated.\")\n",
    "\n",
    "            # Open the PDF file as a buffer and run the pipeline\n",
    "            with open(pdf_full_path, \"rb\") as f:\n",
    "                pdf_buffer = io.BytesIO(f.read())\n",
    "\n",
    "                pipeline_run_result = await pipeline.run(\n",
    "                    pdf_file_buffer=pdf_buffer,\n",
    "                    user_id=TEST_USER_UID,\n",
    "                    original_filename=original_filename,\n",
    "                    doc_type=doc_type\n",
    "                )\n",
    "\n",
    "            # Print the final result from the pipeline run\n",
    "            print(\"\\n--- Pipeline Run Result ---\")\n",
    "            print(f\"Success: {pipeline_run_result.get('success')}\")\n",
    "            print(f\"Message: {pipeline_run_result.get('message')}\")\n",
    "            final_document_id = pipeline_run_result.get('document_id')\n",
    "            print(f\"Document ID: {final_document_id}\")\n",
    "            if pipeline_run_result.get('success'):\n",
    "                 print(f\"Chunks Generated: {pipeline_run_result.get('chunk_count')}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred during pipeline execution: {e}\")\n",
    "            # Print traceback for debugging\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Pipeline execution because Supabase client authentication failed.\")\n",
    "\n",
    "print(\"\\n--- Pipeline Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a641698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Programmatic Verification (Basic) ---\n",
      "Document Record Verification:\n",
      "  Found Document: 70534719-0dd7-4a6e-914d-07f767b71f4b\n",
      "  Status: completed\n",
      "  Filename: 10k_tesla_3_pages.pdf\n",
      "  Type: Annual Report\n",
      "\n",
      "Chunk Verification:\n",
      "  Count of chunks found in DB for this document: 14\n",
      "  Matches chunk count reported by pipeline (14).\n",
      "\n",
      "First Chunk Embedding Verification:\n",
      "  First Chunk ID: c641d252-4cd6-4e77-84ed-723ee561e284\n",
      "  Raw embedding was string, parsed to list.\n",
      "  Embedding exists and is a list (length: 1536).\n",
      "\n",
      "--- Programmatic Verification Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: (Optional but Recommended) Verification in Supabase\n",
    "\n",
    "import json # <-- Import json for parsing embedding string\n",
    "\n",
    "# Verify the results directly in your Supabase Dashboard:\n",
    "# 1. Go to Storage -> financial-pdfs bucket. Check if a folder matching TEST_USER_UID exists\n",
    "#    and contains a subfolder matching the final_document_id (if generated) with the PDF inside.\n",
    "# 2. Go to Table Editor -> documents table. Filter by the final_document_id. Check if the\n",
    "#    record exists, belongs to the correct user_id, and has the expected status ('completed')\n",
    "#    and metadata.\n",
    "# 3. Go to Table Editor -> sections table. Filter by the final_document_id. Check if the\n",
    "#    expected number of sections were created and linked correctly.\n",
    "# 4. Go to Table Editor -> chunks table. Filter by the final_document_id. Check if the\n",
    "#    expected number of chunks were created, linked correctly, and have an 'embedding' column\n",
    "#    that is not null (verifying embeddings were saved).\n",
    "\n",
    "# You can also add code here to query Supabase using the client to verify programmatically.\n",
    "\n",
    "if supabase_client_authenticated and final_document_id:\n",
    "    print(\"\\n--- Programmatic Verification (Basic) ---\")\n",
    "    try:\n",
    "        # Check document record\n",
    "        doc_response = supabase_client_authenticated.table(\"documents\")\\\n",
    "            .select(\"id, status, filename, doc_specific_type\")\\\n",
    "            .eq(\"id\", str(final_document_id))\\\n",
    "            .eq(\"user_id\", str(TEST_USER_UID))\\\n",
    "            .maybe_single()\\\n",
    "            .execute()\n",
    "\n",
    "        if doc_response.data:\n",
    "            print(f\"Document Record Verification:\")\n",
    "            print(f\"  Found Document: {doc_response.data.get('id')}\")\n",
    "            print(f\"  Status: {doc_response.data.get('status')}\")\n",
    "            print(f\"  Filename: {doc_response.data.get('filename')}\")\n",
    "            print(f\"  Type: {doc_response.data.get('doc_specific_type')}\")\n",
    "            assert doc_response.data.get('status') == 'completed', f\"Document status is not 'completed'! Found: '{doc_response.data.get('status')}'\" # Added detail to assertion\n",
    "        else:\n",
    "            print(f\"Verification FAILED: Document record not found for ID: {final_document_id}\")\n",
    "            # Raise assertion here if document must exist\n",
    "            assert False, f\"Document record verification failed: Record not found for ID {final_document_id}\"\n",
    "\n",
    "\n",
    "        # Check chunk count\n",
    "        # Note: RLS applies, so this counts only chunks for the authenticated user AND this document\n",
    "        chunk_count_response = supabase_client_authenticated.table(\"chunks\")\\\n",
    "            .select(\"id\", count=\"exact\")\\\n",
    "            .eq(\"document_id\", str(final_document_id))\\\n",
    "            .execute()\n",
    "\n",
    "        db_chunk_count = chunk_count_response.count\n",
    "        print(f\"\\nChunk Verification:\")\n",
    "        print(f\"  Count of chunks found in DB for this document: {db_chunk_count}\")\n",
    "        # Compare with the count returned by the pipeline if available\n",
    "        pipeline_chunk_count = pipeline_run_result.get('chunk_count')\n",
    "        if pipeline_chunk_count is not None:\n",
    "             assert db_chunk_count == pipeline_chunk_count, \\\n",
    "                 f\"Chunk count mismatch! Pipeline reported {pipeline_chunk_count}, DB has {db_chunk_count}.\"\n",
    "             print(f\"  Matches chunk count reported by pipeline ({pipeline_chunk_count}).\")\n",
    "        else:\n",
    "             print(\"  Pipeline result did not report chunk count for comparison.\")\n",
    "\n",
    "\n",
    "        # Check first chunk embedding (ensure it's not null and is a list after potential parsing)\n",
    "        first_chunk_response = supabase_client_authenticated.table(\"chunks\")\\\n",
    "             .select(\"id, embedding\")\\\n",
    "             .eq(\"document_id\", str(final_document_id))\\\n",
    "             .order(\"chunk_index\")\\\n",
    "             .limit(1)\\\n",
    "             .maybe_single()\\\n",
    "             .execute()\n",
    "\n",
    "        if first_chunk_response.data:\n",
    "             print(\"\\nFirst Chunk Embedding Verification:\")\n",
    "             print(f\"  First Chunk ID: {first_chunk_response.data.get('id')}\")\n",
    "             embedding_value_raw = first_chunk_response.data.get('embedding') # Get the raw value\n",
    "\n",
    "             # --- MODIFICATION START: Parse embedding string if needed ---\n",
    "             assert embedding_value_raw is not None, \"Embedding value is NULL!\"\n",
    "\n",
    "             parsed_embedding_list = None\n",
    "             if isinstance(embedding_value_raw, str):\n",
    "                 # Attempt to parse the string representation '[num, num, ...]'\n",
    "                 try:\n",
    "                     # Use json.loads as it can handle list string format\n",
    "                     parsed_embedding_list = json.loads(embedding_value_raw)\n",
    "                     print(f\"  Raw embedding was string, parsed to list.\")\n",
    "                 except json.JSONDecodeError:\n",
    "                     print(f\"  Error: Could not parse embedding string: {embedding_value_raw[:100]}...\")\n",
    "                     assert False, \"Failed to parse embedding string from database.\" # Fail assertion\n",
    "             elif isinstance(embedding_value_raw, list):\n",
    "                 # If it's already a list (less likely but possible with future library updates)\n",
    "                 parsed_embedding_list = embedding_value_raw\n",
    "                 print(f\"  Raw embedding was already a list.\")\n",
    "             else:\n",
    "                  assert False, f\"Embedding value has unexpected type: {type(embedding_value_raw)}\" # Fail if not string or list\n",
    "\n",
    "             assert isinstance(parsed_embedding_list, list), \"Parsed embedding value is not a list!\"\n",
    "             # --- MODIFICATION END ---\n",
    "\n",
    "             print(f\"  Embedding exists and is a list (length: {len(parsed_embedding_list)}).\")\n",
    "             # Optional: Further check content if needed\n",
    "             # assert all(isinstance(x, (int, float)) for x in parsed_embedding_list), \\\n",
    "             #    \"Parsed embedding list contains non-numeric values\"\n",
    "\n",
    "        elif db_chunk_count > 0: # Only fail if chunks existed but query failed\n",
    "             print(\"  Verification FAILED: Could not retrieve first chunk to verify embedding.\")\n",
    "             assert False, \"Could not retrieve first chunk.\"\n",
    "        else: # No chunks existed in the first place\n",
    "             print(\"  No chunks found to verify embedding (expected, as chunk count is 0).\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- Programmatic Verification Complete ---\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "         print(f\"\\nVERIFICATION FAILED: Assertion Error - {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"\\nAn error occurred during programmatic verification: {e}\")\n",
    "         # Print traceback for debugging other errors\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping programmatic verification as client/document ID is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceb952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: (Optional but Recommended) Cleanup - Delete test data created by this run\n",
    "\n",
    "# Import SupabaseService if not already imported in this scope (needed for BUCKET_NAME)\n",
    "# If Cell 4 imported it, this is technically redundant but safe\n",
    "from src.storage.SupabaseService import SupabaseService\n",
    "\n",
    "print(\"\\n--- Cleanup ---\")\n",
    "# Use the final_document_id obtained from the pipeline run result for cleanup\n",
    "if supabase_client_authenticated and final_document_id:\n",
    "    print(f\"Attempting cleanup for Document ID: {final_document_id}\")\n",
    "\n",
    "    # Instantiate SupabaseService again, passing the authenticated client for cleanup actions\n",
    "    # Although we use the client directly below, instantiating the service is fine too\n",
    "    cleanup_service = SupabaseService(supabase_client=supabase_client_authenticated)\n",
    "\n",
    "    # --- Step 1: Retrieve the Storage Path BEFORE deleting the document ---\n",
    "    retrieved_storage_path: str | None = None\n",
    "    print(\"  Retrieving storage path from document record...\")\n",
    "    try:\n",
    "        doc_path_response = cleanup_service.client.table('documents')\\\n",
    "            .select(\"storage_path\")\\\n",
    "            .eq('id', str(final_document_id))\\\n",
    "            .eq('user_id', str(TEST_USER_UID))\\\n",
    "            .maybe_single()\\\n",
    "            .execute()\n",
    "\n",
    "        if doc_path_response.data and doc_path_response.data.get('storage_path'):\n",
    "            retrieved_storage_path = doc_path_response.data['storage_path']\n",
    "            print(f\"  Found storage path: {retrieved_storage_path}\")\n",
    "        else:\n",
    "            print(\"  Could not retrieve storage path from document record (might be already deleted or missing).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error retrieving storage path: {e}\")\n",
    "\n",
    "\n",
    "    # --- Step 2: Delete Uploaded File from Storage using the RETRIEVED path ---\n",
    "    if retrieved_storage_path:\n",
    "        print(f\"  Deleting file from storage using retrieved path: {retrieved_storage_path}...\")\n",
    "        try:\n",
    "            # Use the path retrieved from the database record\n",
    "            delete_response = cleanup_service.client.storage.from_(SupabaseService.STORAGE_BUCKET_NAME).remove([retrieved_storage_path])\n",
    "            if delete_response: # Check if response indicates items were processed\n",
    "                print(\"  Storage file deletion attempt processed.\")\n",
    "                # supabase-py remove often returns a list of deleted items\n",
    "                deleted_successfully = False\n",
    "                for item in delete_response:\n",
    "                     # Check based on path structure if possible, or just confirm deletion reported\n",
    "                     print(f\"    Deleted item name from response: {item.get('name')}\")\n",
    "                     # A more robust check might involve parsing the path from the response if needed\n",
    "                     deleted_successfully = True # Assume success if response has items\n",
    "                if deleted_successfully:\n",
    "                     print(\"  Storage file likely deleted successfully.\")\n",
    "                else:\n",
    "                     print(\"  Storage file deletion response was empty or did not contain expected item.\")\n",
    "\n",
    "            else:\n",
    "                 print(\"  Storage file deletion response was empty (might mean file not found or already deleted).\")\n",
    "        except Exception as e:\n",
    "             print(f\"  Error deleting storage file: {e}\")\n",
    "             if \"Object not found\" in str(e):\n",
    "                  print(\"  Hint: File was likely already deleted.\")\n",
    "    else:\n",
    "        print(\"  Skipping storage file deletion as path could not be retrieved.\")\n",
    "\n",
    "\n",
    "    # --- Step 3: Delete Document Record (CASCADE should handle sections and chunks) ---\n",
    "    print(\"  Deleting document record from database...\")\n",
    "    try:\n",
    "        # Use the client directly for simplicity in cleanup verification\n",
    "        response = cleanup_service.client.table('documents')\\\n",
    "            .delete()\\\n",
    "            .eq('id', str(final_document_id))\\\n",
    "            .eq('user_id', str(TEST_USER_UID)) \\\n",
    "            .execute()\n",
    "        # Check if delete likely succeeded (response.data usually contains deleted rows)\n",
    "        if response.data and len(response.data) > 0:\n",
    "            print(f\"  Successfully deleted document record: {response.data[0].get('id')}\")\n",
    "        elif response.data and len(response.data) == 0:\n",
    "            print(f\"  Document record {final_document_id} not found (already deleted?).\")\n",
    "        else:\n",
    "             print(f\"  Document deletion might have failed. Response: {response}\") # Print full response for debug\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting document record: {e}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping cleanup as Supabase client or final document ID is not available.\")\n",
    "\n",
    "print(\"\\n--- Cleanup Complete ---\")\n",
    "\n",
    "# Optional: Sign out the user\n",
    "# if supabase_client_authenticated:\n",
    "#    print(\"\\nSigning out test user...\")\n",
    "#    supabase_client_authenticated.auth.sign_out()\n",
    "#    print(\"User signed out.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
