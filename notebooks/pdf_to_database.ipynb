{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c75db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: c:\\Users\\wbrya\\OneDrive\\Documents\\GitHub\\AI-CFO-FYP\n",
      "Current working directory: c:\\Users\\wbrya\\OneDrive\\Documents\\GitHub\\AI-CFO-FYP\\notebooks\n",
      "\n",
      "--- Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup - Add project root to sys.path and install necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import uuid\n",
    "from pathlib import Path # Use pathlib for easier path handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Ensure necessary libraries are installed (re-run if needed)\n",
    "# !pip install -r ../requirements.txt\n",
    "# !pip install supabase python-dotenv pydantic google-genai openai pymupdf chonkie\n",
    "# !pip install -e . # Install local src package\n",
    "\n",
    "print(\"\\n--- Setup Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b73abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported IngestionPipeline and Supabase client.\n",
      "\n",
      "--- Imports Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import necessary modules and the IngestionPipeline\n",
    "from supabase import create_client, Client\n",
    "from src.pipeline import IngestionPipeline # Import the main pipeline class\n",
    "\n",
    "print(\"Imported IngestionPipeline and Supabase client.\")\n",
    "print(\"\\n--- Imports Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98be7e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPABASE_URL Loaded: Yes\n",
      "SUPABASE_ANON_KEY Loaded: Yes\n",
      "TEST_EMAIL Loaded: Yes\n",
      "TEST_PASSWORD Loaded: Yes\n",
      "Target TEST_USER_UID: e222921f-cfdc-4a05-8cf2-aea13004bcf2\n",
      "\n",
      "Initializing Supabase client...\n",
      "Supabase client initialized.\n",
      "\n",
      "Attempting to sign in user: wbryanlai@gmail.com...\n",
      "Sign-in successful for user ID: e222921f-cfdc-4a05-8cf2-aea13004bcf2\n",
      "Logged-in user ID matches target TEST_USER_UID.\n",
      "\n",
      "--- Client Initialization and Authentication Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Environment Variables and Authenticate Test User for Supabase Client\n",
    "load_dotenv() # Load variables from .env file\n",
    "\n",
    "# Get Supabase credentials\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_ANON_KEY\") # Use ANON key\n",
    "\n",
    "# --- Test User Credentials (MUST match the TEST_USER_UID below) ---\n",
    "test_email = os.environ.get(\"TEST_EMAIL\")\n",
    "test_password = os.environ.get(\"TEST_PASSWORD\")\n",
    "# --- Target User UID ---\n",
    "TEST_USER_UID_STR = \"e222921f-cfdc-4a05-8cf2-aea13004bcf2\"\n",
    "TEST_USER_UID = uuid.UUID(TEST_USER_UID_STR)\n",
    "\n",
    "print(f\"SUPABASE_URL Loaded: {'Yes' if supabase_url else 'No'}\")\n",
    "print(f\"SUPABASE_ANON_KEY Loaded: {'Yes' if supabase_key else 'No'}\")\n",
    "print(f\"TEST_EMAIL Loaded: {'Yes' if test_email else 'No'}\")\n",
    "print(f\"TEST_PASSWORD Loaded: {'Yes' if test_password else 'No'}\")\n",
    "print(f\"Target TEST_USER_UID: {TEST_USER_UID}\")\n",
    "\n",
    "# Basic validation\n",
    "if not all([supabase_url, supabase_key, test_email, test_password]):\n",
    "    print(\"\\nError: Missing required Supabase credentials or test user info in .env file.\")\n",
    "    supabase_client_authenticated: Client | None = None\n",
    "else:\n",
    "    try:\n",
    "        # Initialize Supabase client\n",
    "        print(\"\\nInitializing Supabase client...\")\n",
    "        temp_client: Client | None = create_client(supabase_url, supabase_key)\n",
    "        print(\"Supabase client initialized.\")\n",
    "\n",
    "        # --- Authenticate the client ---\n",
    "        print(f\"\\nAttempting to sign in user: {test_email}...\")\n",
    "        response = temp_client.auth.sign_in_with_password(\n",
    "            {\"email\": test_email, \"password\": test_password}\n",
    "        )\n",
    "\n",
    "        if response and response.session and response.user:\n",
    "            print(f\"Sign-in successful for user ID: {response.user.id}\")\n",
    "            if str(response.user.id) == TEST_USER_UID_STR:\n",
    "                 print(\"Logged-in user ID matches target TEST_USER_UID.\")\n",
    "                 # Store the authenticated client for the pipeline\n",
    "                 supabase_client_authenticated = temp_client\n",
    "            else:\n",
    "                print(f\"CRITICAL ERROR: Logged-in user ID ({response.user.id}) does NOT match target ({TEST_USER_UID}).\")\n",
    "                supabase_client_authenticated = None\n",
    "        else:\n",
    "            print(\"Sign-in failed. Response:\", response)\n",
    "            supabase_client_authenticated = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError initializing Supabase client or signing in: {e}\")\n",
    "        supabase_client_authenticated = None\n",
    "\n",
    "print(\"\\n--- Client Initialization and Authentication Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494778d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running the Full Ingestion Pipeline ---\n",
      "Processing file: 10k_tesla_3_pages.pdf (Type: pdf)\n",
      "Owned by User ID: e222921f-cfdc-4a05-8cf2-aea13004bcf2\n",
      "Instantiating IngestionPipeline...\n",
      "SupabaseService initialized with provided client.\n",
      "Initializing IngestionPipeline...\n",
      "Initializing Gemini client with API key: AIz...yQ\n",
      "Initialized OpenAI client with model: text-embedding-3-small\n",
      "Initialized Sectioner.\n",
      "Initializing ChunkingService with RecursiveChunker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wbrya\\OneDrive\\Documents\\GitHub\\AI-CFO-FYP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveChunker initialized with chunk_size=2048, min_chars=24\n",
      "Initialized EmbeddingService using model: text-embedding-3-small\n",
      "IngestionPipeline initialized with all services.\n",
      "Pipeline instantiated.\n",
      "\n",
      "--- Starting Ingestion Pipeline for: 10k_tesla_3_pages.pdf (User: e222921f-cfdc-4a05-8cf2-aea13004bcf2) ---\n",
      "\n",
      "Step 1: Parsing PDF to Markdown...\n",
      "PDF has 3 pages\n",
      "Rendering page 1/3\n",
      "Rendering page 2/3\n",
      "Rendering page 3/3\n",
      "Starting page annotation with max 3 concurrent workers...\n",
      "Page 1: Annotating (Attempt 1/6)\n",
      "Page 2: Annotating (Attempt 1/6)\n",
      "Page 3: Annotating (Attempt 1/6)\n",
      "Page 2: Annotation successful.\n",
      "Page 1: Annotation successful.\n",
      "Page 3: Annotation successful.\n",
      "Parsing successful (3 pages). Markdown length: 10035\n",
      "\n",
      "Step 2: Extracting Document Metadata...\n",
      "Sending text snippet to LLM for structured metadata extraction...\n",
      "Structured metadata extraction attempted.\n",
      "Metadata extraction successful.\n",
      "  Extracted Type: Annual Report\n",
      "  Extracted Company: Tesla, Inc.\n",
      "\n",
      "Step 3: Uploading Original PDF (using temp ID for path: ed6ee9c9-f40b-4090-a98c-906ecea74d0c)...\n",
      "Attempting to upload PDF to storage path: e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf\n",
      "Supabase storage upload response for e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf\n",
      "PDF successfully uploaded to: e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf\n",
      "Original PDF uploaded successfully to: e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf\n",
      "\n",
      "Step 4: Saving Document Record to Database...\n",
      "Saving document record for: 10k_tesla_3_pages.pdf (User: e222921f-cfdc-4a05-8cf2-aea13004bcf2)\n",
      "Document record saved successfully. Document ID: a5960ced-b9f0-434b-bfe9-bdf097e0e0b0\n",
      "Document record saved successfully. Document ID: a5960ced-b9f0-434b-bfe9-bdf097e0e0b0\n",
      "\n",
      "Step 5: Sectioning Markdown Content...\n",
      "Sectioning markdown content (151 lines)...\n",
      "Finalized section 0: 'Document Start'\n",
      "Finalized section 1: 'UNITED STATES'\n",
      "Finalized section 2: 'SECURITIES AND EXCHANGE COMMISSION'\n",
      "Finalized section 3: 'Washington, D.C. 20549'\n",
      "Finalized section 4: 'FORM 10-K'\n",
      "Finalized section 5: 'Tesla, Inc.'\n",
      "Finalized last section 6: 'Securities registered pursuant to Section 12(b) of the Act:'\n",
      "Sectioning complete. Created 7 sections.\n",
      "\n",
      "Step 6: Saving Sections to Database...\n",
      "Saving batch of 7 section records...\n",
      "Batch of 7 sections saved successfully.\n",
      "Sections saved successfully (7 sections).\n",
      "\n",
      "Step 7: Chunking Sections...\n",
      "Chunking 7 sections...\n",
      "Chunking section 'Document Start' (Index: 0)...\n",
      "Chunked section 'Document Start' into 1 chunks.\n",
      "Chunking section 'UNITED STATES' (Index: 1)...\n",
      "Chunked section 'UNITED STATES' into 1 chunks.\n",
      "Chunking section 'SECURITIES AND EXCHANGE COMMISSION' (Index: 2)...\n",
      "Chunked section 'SECURITIES AND EXCHANGE COMMISSION' into 1 chunks.\n",
      "Chunking section 'Washington, D.C. 20549' (Index: 3)...\n",
      "Chunked section 'Washington, D.C. 20549' into 1 chunks.\n",
      "Chunking section 'FORM 10-K' (Index: 4)...\n",
      "Chunked section 'FORM 10-K' into 1 chunks.\n",
      "Chunking section 'Tesla, Inc.' (Index: 5)...\n",
      "Chunked section 'Tesla, Inc.' into 1 chunks.\n",
      "Chunking section 'Securities registered pursuant to Section 12(b) of the Act:' (Index: 6)...\n",
      "Chunked section 'Securities registered pursuant to Section 12(b) of the Act:' into 3 chunks.\n",
      "ChunkingService completed. Total chunks generated: 9\n",
      "\n",
      "Step 8: Generating Embeddings...\n",
      "Generating embeddings for 9 chunks using model: text-embedding-3-small...\n",
      "Prepared 9 augmented texts for embedding.\n",
      "Successfully added embeddings to 9 chunks.\n",
      "Embeddings generated successfully.\n",
      "\n",
      "Step 9: Saving Chunks with Embeddings to Database...\n",
      "Saving batch of 9 chunk records...\n",
      "Attempting bulk insert of 9 chunks...\n",
      "Batch of 9 chunks likely saved successfully.\n",
      "Chunks saved successfully.\n",
      "\n",
      "Step 10: Updating Document Status to Completed...\n",
      "Updating status for document a5960ced-b9f0-434b-bfe9-bdf097e0e0b0 to 'completed'\n",
      "Document status updated successfully.\n",
      "\n",
      "--- Ingestion Pipeline Successfully Completed for 10k_tesla_3_pages.pdf in 15.24 seconds ---\n",
      "\n",
      "--- Pipeline Run Result ---\n",
      "Success: True\n",
      "Message: Document processed and ingested successfully.\n",
      "Document ID: a5960ced-b9f0-434b-bfe9-bdf097e0e0b0\n",
      "Chunks Generated: 9\n",
      "\n",
      "--- Pipeline Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Test File Path and Run the Ingestion Pipeline\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the PDF file you want to process, relative to the project root\n",
    "# Make sure this file exists!\n",
    "from src.services.SupabaseService import SupabaseService\n",
    "\n",
    "\n",
    "PDF_RELATIVE_PATH = \"data/source_pdfs/10k_tesla_3_pages.pdf\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Variable to store the result of the pipeline run\n",
    "pipeline_run_result = None\n",
    "final_document_id = None\n",
    "\n",
    "# Proceed only if we have an authenticated Supabase client\n",
    "if supabase_client_authenticated:\n",
    "    print(\"\\n--- Running the Full Ingestion Pipeline ---\")\n",
    "\n",
    "    # Construct the full path to the PDF file\n",
    "    pdf_full_path = Path(project_root) / PDF_RELATIVE_PATH\n",
    "    original_filename = pdf_full_path.name\n",
    "    doc_type = pdf_full_path.suffix[1:].lower() # Get 'pdf' from '.pdf'\n",
    "\n",
    "    if not pdf_full_path.is_file():\n",
    "        print(f\"Error: Test PDF file not found at {pdf_full_path}\")\n",
    "    else:\n",
    "        print(f\"Processing file: {original_filename} (Type: {doc_type})\")\n",
    "        print(f\"Owned by User ID: {TEST_USER_UID}\")\n",
    "\n",
    "        try:\n",
    "            # Instantiate the IngestionPipeline\n",
    "            # We can pass the authenticated client to the SupabaseService instance it creates\n",
    "            # (or modify pipeline __init__ to accept and pass clients)\n",
    "            # For now, we rely on SupabaseService creating its own client,\n",
    "            # which will pick up the auth context set by sign_in_with_password\n",
    "            # on the shared supabase_py instance state (this is how supabase-py often works).\n",
    "            # A more robust approach would be explicit client passing.\n",
    "            print(\"Instantiating IngestionPipeline...\")\n",
    "            pipeline = IngestionPipeline(\n",
    "                 # Explicitly pass the authenticated client to SupabaseService if preferred:\n",
    "                 supabase_service=SupabaseService(supabase_client=supabase_client_authenticated)\n",
    "                 # Otherwise, rely on the default __init__ behavior.\n",
    "            )\n",
    "            print(\"Pipeline instantiated.\")\n",
    "\n",
    "            # Open the PDF file as a buffer and run the pipeline\n",
    "            with open(pdf_full_path, \"rb\") as f:\n",
    "                pdf_buffer = io.BytesIO(f.read())\n",
    "\n",
    "                pipeline_run_result = await pipeline.run(\n",
    "                    pdf_file_buffer=pdf_buffer,\n",
    "                    user_id=TEST_USER_UID,\n",
    "                    original_filename=original_filename,\n",
    "                    doc_type=doc_type\n",
    "                )\n",
    "\n",
    "            # Print the final result from the pipeline run\n",
    "            print(\"\\n--- Pipeline Run Result ---\")\n",
    "            print(f\"Success: {pipeline_run_result.get('success')}\")\n",
    "            print(f\"Message: {pipeline_run_result.get('message')}\")\n",
    "            final_document_id = pipeline_run_result.get('document_id')\n",
    "            print(f\"Document ID: {final_document_id}\")\n",
    "            if pipeline_run_result.get('success'):\n",
    "                 print(f\"Chunks Generated: {pipeline_run_result.get('chunk_count')}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred during pipeline execution: {e}\")\n",
    "            # Print traceback for debugging\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Pipeline execution because Supabase client authentication failed.\")\n",
    "\n",
    "print(\"\\n--- Pipeline Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a641698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Programmatic Verification (Basic) ---\n",
      "Document Record Verification:\n",
      "  Found Document: a5960ced-b9f0-434b-bfe9-bdf097e0e0b0\n",
      "  Status: completed\n",
      "  Filename: 10k_tesla_3_pages.pdf\n",
      "  Type: Annual Report\n",
      "\n",
      "Chunk Verification:\n",
      "  Count of chunks found in DB for this document: 9\n",
      "  Matches chunk count reported by pipeline (9).\n",
      "\n",
      "First Chunk Embedding Verification:\n",
      "  First Chunk ID: f2460100-f894-431d-af2e-fee32f998eb5\n",
      "  Raw embedding was string, parsed to list.\n",
      "  Embedding exists and is a list (length: 1536).\n",
      "\n",
      "--- Programmatic Verification Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: (Optional but Recommended) Verification in Supabase\n",
    "\n",
    "import json # <-- Import json for parsing embedding string\n",
    "\n",
    "# Verify the results directly in your Supabase Dashboard:\n",
    "# 1. Go to Storage -> financial-pdfs bucket. Check if a folder matching TEST_USER_UID exists\n",
    "#    and contains a subfolder matching the final_document_id (if generated) with the PDF inside.\n",
    "# 2. Go to Table Editor -> documents table. Filter by the final_document_id. Check if the\n",
    "#    record exists, belongs to the correct user_id, and has the expected status ('completed')\n",
    "#    and metadata.\n",
    "# 3. Go to Table Editor -> sections table. Filter by the final_document_id. Check if the\n",
    "#    expected number of sections were created and linked correctly.\n",
    "# 4. Go to Table Editor -> chunks table. Filter by the final_document_id. Check if the\n",
    "#    expected number of chunks were created, linked correctly, and have an 'embedding' column\n",
    "#    that is not null (verifying embeddings were saved).\n",
    "\n",
    "# You can also add code here to query Supabase using the client to verify programmatically.\n",
    "\n",
    "if supabase_client_authenticated and final_document_id:\n",
    "    print(\"\\n--- Programmatic Verification (Basic) ---\")\n",
    "    try:\n",
    "        # Check document record\n",
    "        doc_response = supabase_client_authenticated.table(\"documents\")\\\n",
    "            .select(\"id, status, filename, doc_specific_type\")\\\n",
    "            .eq(\"id\", str(final_document_id))\\\n",
    "            .eq(\"user_id\", str(TEST_USER_UID))\\\n",
    "            .maybe_single()\\\n",
    "            .execute()\n",
    "\n",
    "        if doc_response.data:\n",
    "            print(f\"Document Record Verification:\")\n",
    "            print(f\"  Found Document: {doc_response.data.get('id')}\")\n",
    "            print(f\"  Status: {doc_response.data.get('status')}\")\n",
    "            print(f\"  Filename: {doc_response.data.get('filename')}\")\n",
    "            print(f\"  Type: {doc_response.data.get('doc_specific_type')}\")\n",
    "            assert doc_response.data.get('status') == 'completed', f\"Document status is not 'completed'! Found: '{doc_response.data.get('status')}'\" # Added detail to assertion\n",
    "        else:\n",
    "            print(f\"Verification FAILED: Document record not found for ID: {final_document_id}\")\n",
    "            # Raise assertion here if document must exist\n",
    "            assert False, f\"Document record verification failed: Record not found for ID {final_document_id}\"\n",
    "\n",
    "\n",
    "        # Check chunk count\n",
    "        # Note: RLS applies, so this counts only chunks for the authenticated user AND this document\n",
    "        chunk_count_response = supabase_client_authenticated.table(\"chunks\")\\\n",
    "            .select(\"id\", count=\"exact\")\\\n",
    "            .eq(\"document_id\", str(final_document_id))\\\n",
    "            .execute()\n",
    "\n",
    "        db_chunk_count = chunk_count_response.count\n",
    "        print(f\"\\nChunk Verification:\")\n",
    "        print(f\"  Count of chunks found in DB for this document: {db_chunk_count}\")\n",
    "        # Compare with the count returned by the pipeline if available\n",
    "        pipeline_chunk_count = pipeline_run_result.get('chunk_count')\n",
    "        if pipeline_chunk_count is not None:\n",
    "             assert db_chunk_count == pipeline_chunk_count, \\\n",
    "                 f\"Chunk count mismatch! Pipeline reported {pipeline_chunk_count}, DB has {db_chunk_count}.\"\n",
    "             print(f\"  Matches chunk count reported by pipeline ({pipeline_chunk_count}).\")\n",
    "        else:\n",
    "             print(\"  Pipeline result did not report chunk count for comparison.\")\n",
    "\n",
    "\n",
    "        # Check first chunk embedding (ensure it's not null and is a list after potential parsing)\n",
    "        first_chunk_response = supabase_client_authenticated.table(\"chunks\")\\\n",
    "             .select(\"id, embedding\")\\\n",
    "             .eq(\"document_id\", str(final_document_id))\\\n",
    "             .order(\"chunk_index\")\\\n",
    "             .limit(1)\\\n",
    "             .maybe_single()\\\n",
    "             .execute()\n",
    "\n",
    "        if first_chunk_response.data:\n",
    "             print(\"\\nFirst Chunk Embedding Verification:\")\n",
    "             print(f\"  First Chunk ID: {first_chunk_response.data.get('id')}\")\n",
    "             embedding_value_raw = first_chunk_response.data.get('embedding') # Get the raw value\n",
    "\n",
    "             # --- MODIFICATION START: Parse embedding string if needed ---\n",
    "             assert embedding_value_raw is not None, \"Embedding value is NULL!\"\n",
    "\n",
    "             parsed_embedding_list = None\n",
    "             if isinstance(embedding_value_raw, str):\n",
    "                 # Attempt to parse the string representation '[num, num, ...]'\n",
    "                 try:\n",
    "                     # Use json.loads as it can handle list string format\n",
    "                     parsed_embedding_list = json.loads(embedding_value_raw)\n",
    "                     print(f\"  Raw embedding was string, parsed to list.\")\n",
    "                 except json.JSONDecodeError:\n",
    "                     print(f\"  Error: Could not parse embedding string: {embedding_value_raw[:100]}...\")\n",
    "                     assert False, \"Failed to parse embedding string from database.\" # Fail assertion\n",
    "             elif isinstance(embedding_value_raw, list):\n",
    "                 # If it's already a list (less likely but possible with future library updates)\n",
    "                 parsed_embedding_list = embedding_value_raw\n",
    "                 print(f\"  Raw embedding was already a list.\")\n",
    "             else:\n",
    "                  assert False, f\"Embedding value has unexpected type: {type(embedding_value_raw)}\" # Fail if not string or list\n",
    "\n",
    "             assert isinstance(parsed_embedding_list, list), \"Parsed embedding value is not a list!\"\n",
    "             # --- MODIFICATION END ---\n",
    "\n",
    "             print(f\"  Embedding exists and is a list (length: {len(parsed_embedding_list)}).\")\n",
    "             # Optional: Further check content if needed\n",
    "             # assert all(isinstance(x, (int, float)) for x in parsed_embedding_list), \\\n",
    "             #    \"Parsed embedding list contains non-numeric values\"\n",
    "\n",
    "        elif db_chunk_count > 0: # Only fail if chunks existed but query failed\n",
    "             print(\"  Verification FAILED: Could not retrieve first chunk to verify embedding.\")\n",
    "             assert False, \"Could not retrieve first chunk.\"\n",
    "        else: # No chunks existed in the first place\n",
    "             print(\"  No chunks found to verify embedding (expected, as chunk count is 0).\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- Programmatic Verification Complete ---\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "         print(f\"\\nVERIFICATION FAILED: Assertion Error - {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"\\nAn error occurred during programmatic verification: {e}\")\n",
    "         # Print traceback for debugging other errors\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping programmatic verification as client/document ID is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ceb952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleanup ---\n",
      "Attempting cleanup for Document ID: a5960ced-b9f0-434b-bfe9-bdf097e0e0b0\n",
      "SupabaseService initialized with provided client.\n",
      "  Retrieving storage path from document record...\n",
      "  Found storage path: e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf\n",
      "  Deleting file from storage using retrieved path: e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf...\n",
      "  Storage file deletion attempt processed.\n",
      "    Deleted item name from response: e222921f-cfdc-4a05-8cf2-aea13004bcf2/ed6ee9c9-f40b-4090-a98c-906ecea74d0c/10k_tesla_3_pages.pdf\n",
      "  Storage file likely deleted successfully.\n",
      "  Deleting document record from database...\n",
      "  Successfully deleted document record: a5960ced-b9f0-434b-bfe9-bdf097e0e0b0\n",
      "\n",
      "--- Cleanup Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: (Optional but Recommended) Cleanup - Delete test data created by this run\n",
    "\n",
    "# Import SupabaseService if not already imported in this scope (needed for BUCKET_NAME)\n",
    "# If Cell 4 imported it, this is technically redundant but safe\n",
    "from src.services.SupabaseService import SupabaseService\n",
    "\n",
    "print(\"\\n--- Cleanup ---\")\n",
    "# Use the final_document_id obtained from the pipeline run result for cleanup\n",
    "if supabase_client_authenticated and final_document_id:\n",
    "    print(f\"Attempting cleanup for Document ID: {final_document_id}\")\n",
    "\n",
    "    # Instantiate SupabaseService again, passing the authenticated client for cleanup actions\n",
    "    # Although we use the client directly below, instantiating the service is fine too\n",
    "    cleanup_service = SupabaseService(supabase_client=supabase_client_authenticated)\n",
    "\n",
    "    # --- Step 1: Retrieve the Storage Path BEFORE deleting the document ---\n",
    "    retrieved_storage_path: str | None = None\n",
    "    print(\"  Retrieving storage path from document record...\")\n",
    "    try:\n",
    "        doc_path_response = cleanup_service.client.table('documents')\\\n",
    "            .select(\"storage_path\")\\\n",
    "            .eq('id', str(final_document_id))\\\n",
    "            .eq('user_id', str(TEST_USER_UID))\\\n",
    "            .maybe_single()\\\n",
    "            .execute()\n",
    "\n",
    "        if doc_path_response.data and doc_path_response.data.get('storage_path'):\n",
    "            retrieved_storage_path = doc_path_response.data['storage_path']\n",
    "            print(f\"  Found storage path: {retrieved_storage_path}\")\n",
    "        else:\n",
    "            print(\"  Could not retrieve storage path from document record (might be already deleted or missing).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error retrieving storage path: {e}\")\n",
    "\n",
    "\n",
    "    # --- Step 2: Delete Uploaded File from Storage using the RETRIEVED path ---\n",
    "    if retrieved_storage_path:\n",
    "        print(f\"  Deleting file from storage using retrieved path: {retrieved_storage_path}...\")\n",
    "        try:\n",
    "            # Use the path retrieved from the database record\n",
    "            delete_response = cleanup_service.client.storage.from_(SupabaseService.STORAGE_BUCKET_NAME).remove([retrieved_storage_path])\n",
    "            if delete_response: # Check if response indicates items were processed\n",
    "                print(\"  Storage file deletion attempt processed.\")\n",
    "                # supabase-py remove often returns a list of deleted items\n",
    "                deleted_successfully = False\n",
    "                for item in delete_response:\n",
    "                     # Check based on path structure if possible, or just confirm deletion reported\n",
    "                     print(f\"    Deleted item name from response: {item.get('name')}\")\n",
    "                     # A more robust check might involve parsing the path from the response if needed\n",
    "                     deleted_successfully = True # Assume success if response has items\n",
    "                if deleted_successfully:\n",
    "                     print(\"  Storage file likely deleted successfully.\")\n",
    "                else:\n",
    "                     print(\"  Storage file deletion response was empty or did not contain expected item.\")\n",
    "\n",
    "            else:\n",
    "                 print(\"  Storage file deletion response was empty (might mean file not found or already deleted).\")\n",
    "        except Exception as e:\n",
    "             print(f\"  Error deleting storage file: {e}\")\n",
    "             if \"Object not found\" in str(e):\n",
    "                  print(\"  Hint: File was likely already deleted.\")\n",
    "    else:\n",
    "        print(\"  Skipping storage file deletion as path could not be retrieved.\")\n",
    "\n",
    "\n",
    "    # --- Step 3: Delete Document Record (CASCADE should handle sections and chunks) ---\n",
    "    print(\"  Deleting document record from database...\")\n",
    "    try:\n",
    "        # Use the client directly for simplicity in cleanup verification\n",
    "        response = cleanup_service.client.table('documents')\\\n",
    "            .delete()\\\n",
    "            .eq('id', str(final_document_id))\\\n",
    "            .eq('user_id', str(TEST_USER_UID)) \\\n",
    "            .execute()\n",
    "        # Check if delete likely succeeded (response.data usually contains deleted rows)\n",
    "        if response.data and len(response.data) > 0:\n",
    "            print(f\"  Successfully deleted document record: {response.data[0].get('id')}\")\n",
    "        elif response.data and len(response.data) == 0:\n",
    "            print(f\"  Document record {final_document_id} not found (already deleted?).\")\n",
    "        else:\n",
    "             print(f\"  Document deletion might have failed. Response: {response}\") # Print full response for debug\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error deleting document record: {e}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping cleanup as Supabase client or final document ID is not available.\")\n",
    "\n",
    "print(\"\\n--- Cleanup Complete ---\")\n",
    "\n",
    "# Optional: Sign out the user\n",
    "# if supabase_client_authenticated:\n",
    "#    print(\"\\nSigning out test user...\")\n",
    "#    supabase_client_authenticated.auth.sign_out()\n",
    "#    print(\"User signed out.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
