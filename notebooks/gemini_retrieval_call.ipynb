{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1494fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "     sys.path.insert(0, project_root)\n",
    "\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "from google.genai import types\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "from src.llm.tools.ChunkRetriever import RetrievalService\n",
    "from src.llm.OpenAIClient import OpenAIClient\n",
    "from src.llm.GeminiClient import GeminiClient\n",
    "from src.storage.SupabaseService import SupabaseService\n",
    "from src.helper.llm_helper_chat import create_final_answer_instructions, print_final_formatted_answer, serialize_conversation_history, format_chunks_for_llm\n",
    "\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "YOUR_APP_DOMAIN = \"www.stackifier.com\"\n",
    "\n",
    "# --- SETUP FUNCTION ---\n",
    "def initialize_clients_and_authenticate_for_test() -> tuple[OpenAIClient, GeminiClient, SupabaseService, str, Client, RetrievalService]:\n",
    "    \"\"\"\n",
    "    Loads .env, initializes clients (OpenAI, Gemini, Supabase),\n",
    "    authenticates the test user, initializes RetrievalService,\n",
    "    and returns the clients, user ID, and services.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    TEST_EMAIL = os.environ.get(\"TEST_EMAIL\")\n",
    "    TEST_PASSWORD = os.environ.get(\"TEST_PASSWORD\")\n",
    "    if not TEST_EMAIL or not TEST_PASSWORD:\n",
    "        raise ValueError(\"TEST_EMAIL and TEST_PASSWORD must be set in your .env file.\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- Initializing clients, authenticating, and setting up services ---\")\n",
    "        # Initialize dependent clients first\n",
    "        openai_client_local = OpenAIClient()\n",
    "        gemini_client_local = GeminiClient() \n",
    "\n",
    "        supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "        supabase_key = os.environ.get(\"SUPABASE_ANON_KEY\")\n",
    "        if not supabase_url or not supabase_key:\n",
    "            raise ValueError(\"SUPABASE_URL and SUPABASE_ANON_KEY must be set in your .env file.\")\n",
    "\n",
    "        # Authenticate Supabase client\n",
    "        auth_client_local = create_client(supabase_url, supabase_key)\n",
    "        print(\"Supabase client created.\")\n",
    "\n",
    "        print(f\"Attempting to sign in with email: {TEST_EMAIL}\")\n",
    "        auth_response = auth_client_local.auth.sign_in_with_password(\n",
    "            {\"email\": TEST_EMAIL, \"password\": TEST_PASSWORD}\n",
    "        )\n",
    "\n",
    "        if not auth_response or not auth_response.user:\n",
    "            error_detail = auth_response.error.message if hasattr(auth_response, 'error') and auth_response.error else \"Unknown authentication error\"\n",
    "            raise ConnectionError(f\"Supabase authentication failed: {error_detail}. Check credentials and Supabase Auth settings.\")\n",
    "\n",
    "        authenticated_user_id_str_local = str(auth_response.user.id)\n",
    "        print(f\"Authentication successful. User ID: {authenticated_user_id_str_local}\")\n",
    "\n",
    "        # Initialize Services using the clients\n",
    "        supabase_service_local = SupabaseService(supabase_client=auth_client_local)\n",
    "        # MODIFIED: Initialize RetrievalService\n",
    "        retrieval_service_local = RetrievalService(\n",
    "            openai_client=openai_client_local,\n",
    "            supabase_service=supabase_service_local\n",
    "        )\n",
    "\n",
    "        print(\"Clients and Services initialized and authenticated.\")\n",
    "\n",
    "        # MODIFIED: Return the new service instance\n",
    "        return openai_client_local, gemini_client_local, supabase_service_local, authenticated_user_id_str_local, auth_client_local, retrieval_service_local\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Initialization or Authentication Error: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# --- MAIN QUERY PROCESSING PIPELINE FUNCTION ---\n",
    "def run_financial_query_pipeline(\n",
    "    user_query_text: str,\n",
    "    initial_conv_history: list,\n",
    "    gemini_client_instance: GeminiClient,\n",
    "    auth_user_id: str,\n",
    "    tool_definition: types.Tool,\n",
    "    model_name_to_use: str,\n",
    "    retrieval_service_instance: RetrievalService\n",
    ") -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Manages the multi-turn conversation with Gemini to answer a financial query,\n",
    "    potentially using the retrieve_financial_chunks tool via the RetrievalService.\n",
    "    Returns the final answer text and the updated conversation history.\n",
    "    \"\"\"\n",
    "    current_conv_history = list(initial_conv_history)\n",
    "\n",
    "    try:\n",
    "        # 1. First call to LLM: Send user query and retrieval tool definition\n",
    "        print(f\"\\n--- Sending initial query to Gemini ({model_name_to_use}) ---\")\n",
    "        current_conv_history.append(types.Content(role=\"user\", parts=[types.Part(text=user_query_text)]))\n",
    "        print(f\"  Conversation History before first call:\\n{json.dumps(serialize_conversation_history(current_conv_history), indent=2)}\")\n",
    "\n",
    "        response = gemini_client_instance.client.models.generate_content(\n",
    "            model=model_name_to_use,\n",
    "            contents=current_conv_history,\n",
    "            config=types.GenerateContentConfig(\n",
    "                tools=[tool_definition],\n",
    "                automatic_function_calling= {\"disable\": True},\n",
    "                tool_config= {\"function_calling_config\": {\"mode\": \"any\"}},\n",
    "                temperature=0\n",
    "                )\n",
    "        )\n",
    "        print(f\"\\n--- Received response from first Gemini call ---\")\n",
    "\n",
    "        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:\n",
    "            error_msg = \"Error: Unexpected response structure or no candidates/parts from Gemini's first call.\"\n",
    "            print(error_msg)\n",
    "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback: print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "            return error_msg, current_conv_history\n",
    "\n",
    "        model_response_content = response.candidates[0].content\n",
    "        message_part = model_response_content.parts[0]\n",
    "        current_conv_history.append(model_response_content)\n",
    "        print(f\"\\n  Model's response content (first call) added to history.\")\n",
    "\n",
    "        # 2. Check if LLM requested a function call\n",
    "        if hasattr(message_part, 'function_call') and message_part.function_call:\n",
    "            function_call = message_part.function_call\n",
    "            print(f\"\\n--- Gemini requested function call: '{function_call.name}' ---\")\n",
    "            tool_args = dict(function_call.args)\n",
    "            print(f\"  Raw Arguments from LLM: {tool_args}\")\n",
    "\n",
    "            if function_call.name == \"retrieve_financial_chunks\":\n",
    "                print(\"  Recognized 'retrieve_financial_chunks' call.\")\n",
    "                \n",
    "                # Pass the user_id and LLM provided args, but NOT the clients\n",
    "                function_result_json = retrieval_service_instance.retrieve_chunks(\n",
    "                    user_id=auth_user_id, # Pass user_id explicitly\n",
    "                    **tool_args # Unpack the args provided by the LLM\n",
    "                )\n",
    "\n",
    "                print(f\"\\n--- Finished executing RetrievalService.retrieve_chunks ---\")\n",
    "                print(f\"  Function result (JSON string, first 500 chars):\\n{function_result_json[:500]}...\")\n",
    "\n",
    "                print(\"\\n--- Preparing enriched context and instructions for final Gemini call ---\")\n",
    "                try:\n",
    "                    function_response_data = json.loads(function_result_json)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding function result JSON: {function_result_json[:200]}...\")\n",
    "                    function_response_data = {\"error\": \"Invalid JSON from tool.\"}\n",
    "\n",
    "                function_response_part = types.Part.from_function_response(\n",
    "                    name=function_call.name,\n",
    "                    response={\"result\": function_response_data}\n",
    "                )\n",
    "                current_conv_history.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "                print(f\"  Raw function response part added to history.\")\n",
    "\n",
    "                # Assuming format_chunks_for_llm can handle the JSON string output\n",
    "                formatted_snippets_text = format_chunks_for_llm(function_result_json)\n",
    "                final_instructions_text = create_final_answer_instructions(user_query_text, formatted_snippets_text, YOUR_APP_DOMAIN)\n",
    "                current_conv_history.append(types.Content(role=\"user\", parts=[types.Part(text=final_instructions_text)]))\n",
    "                print(f\"  Formatted snippets and citation instructions added to history.\")\n",
    "                print(f\"\\n  Conversation History before second call (final answer generation):\\n{json.dumps(serialize_conversation_history(current_conv_history), indent=2)}\")\n",
    "\n",
    "                final_response = gemini_client_instance.client.models.generate_content(\n",
    "                    model=model_name_to_use,\n",
    "                    contents=current_conv_history\n",
    "                )\n",
    "                print(f\"\\n--- Received response from second Gemini call (Final Answer) ---\")\n",
    "\n",
    "                if final_response.candidates and final_response.candidates[0].content and final_response.candidates[0].content.parts:\n",
    "                    final_model_response_content = final_response.candidates[0].content\n",
    "                    current_conv_history.append(final_model_response_content)\n",
    "                    return final_response.text, current_conv_history\n",
    "                else:\n",
    "                    error_msg = \"Error: No final response text found after sending function result.\"\n",
    "                    print(error_msg)\n",
    "                    if hasattr(final_response, 'prompt_feedback') and final_response.prompt_feedback: print(f\"Final Response Prompt Feedback: {final_response.prompt_feedback}\")\n",
    "                    return error_msg, current_conv_history\n",
    "            else:\n",
    "                warning_msg = f\"Warning: LLM requested unknown function '{function_call.name}'. Stopping execution due to unknown function call.\"\n",
    "                print(warning_msg)\n",
    "                return warning_msg, current_conv_history\n",
    "        else:\n",
    "            print(\"\\n--- Gemini decided to answer directly (No Function Call Requested) ---\")\n",
    "            if hasattr(message_part, 'text') and message_part.text is not None:\n",
    "                print(message_part.text)\n",
    "                return message_part.text, current_conv_history\n",
    "            else:\n",
    "                no_text_msg = \"No text response found in the initial call and no function call made. Stopping execution after direct answer or unexpected initial response.\"\n",
    "                print(no_text_msg)\n",
    "                return no_text_msg, current_conv_history\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nAn unexpected error occurred during the Gemini interaction: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        return error_msg, current_conv_history\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initalize all variables and services\n",
    "    openai_client_main, gemini_client_main, supabase_service_main, authenticated_user_id_str_main, auth_client_main, retrieval_service_main = initialize_clients_and_authenticate_for_test()\n",
    "    \n",
    "    # User Query and Conversation History\n",
    "    t1 = time.time()\n",
    "    user_query_main = \"Whats the Gross Carrying Amount for Total intangible assets for tesla in 2021? Create a report of tesla for 2021 in markdown i can copy.\"\n",
    "    conversation_history_main = [] # Initialize fresh for each run, or load if continuing\n",
    "\n",
    "    print(f\"\\n--- User Query ---\")\n",
    "    print(user_query_main)\n",
    "\n",
    "    # Tool and Model Configuration\n",
    "    retrieval_tool_main = RetrievalService.get_tool_declaration()\n",
    "    gemini_model_name_main = \"gemini-2.0-flash\"\n",
    "\n",
    "    # Run the main processing pipeline\n",
    "    final_answer_text, updated_history = run_financial_query_pipeline(\n",
    "        user_query_text=user_query_main,\n",
    "        initial_conv_history=conversation_history_main,\n",
    "        gemini_client_instance=gemini_client_main, \n",
    "        auth_user_id=authenticated_user_id_str_main,\n",
    "        tool_definition=retrieval_tool_main,\n",
    "        model_name_to_use=gemini_model_name_main,\n",
    "        retrieval_service_instance=retrieval_service_main\n",
    "    )\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Update conversation history if you plan to continue the conversation\n",
    "    conversation_history_main = updated_history\n",
    "\n",
    "    # Optionally, print the full conversation history for debugging\n",
    "    print(\"\\n--- Full Conversation History (Serialized) ---\")\n",
    "    print(json.dumps(serialize_conversation_history(conversation_history_main), indent=2))\n",
    "    print(f\"\\n[TIMER] TOTAL ELAPSED: {(t2 - t1):.2f}s\")\n",
    "    \n",
    "    # Print the final answer\n",
    "    print_final_formatted_answer(final_answer_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
