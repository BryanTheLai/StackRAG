{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1494fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import uuid\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "from google.genai import types\n",
    "import traceback\n",
    "\n",
    "from src.llm.OpenAIClient import OpenAIClient\n",
    "from src.llm.GeminiClient import GeminiClient\n",
    "from src.storage.SupabaseService import SupabaseService\n",
    "from src.enums import FinancialDocSpecificType\n",
    "from src.prompts.prompt_manager import PromptManager\n",
    "from src.helper.llm_helper_chat import print_final_formatted_answer, serialize_conversation_history, format_chunks_for_llm\n",
    "\n",
    "\n",
    "# --- Global Client Variables ---\n",
    "openai_client: OpenAIClient = None\n",
    "gemini_client: GeminiClient = None # Main pipeline function will take this as an argument\n",
    "supabase_service: SupabaseService = None\n",
    "authenticated_user_id_str: str = None\n",
    "auth_client: Client = None # Supabase auth client\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "YOUR_APP_DOMAIN = \"www.stackifier.com\"\n",
    "\n",
    "# --- PROMPT FUNCTION: Craft instructions for final answer + citation links ---\n",
    "def create_final_answer_instructions(user_original_query: str, formatted_snippets_text: str) -> str:\n",
    "    instructions = PromptManager.get_prompt(\n",
    "        \"citation_answer\", user_original_query = user_original_query,  formatted_snippets_text = formatted_snippets_text, YOUR_APP_DOMAIN = YOUR_APP_DOMAIN\n",
    "    )\n",
    "    return instructions\n",
    "\n",
    "# --- TOOL FUNCTION: Retrieve Financial Chunks ---\n",
    "def retrieve_financial_chunks(\n",
    "    query_text: str,\n",
    "    user_id: str, # This user_id is injected by the calling pipeline, not from LLM\n",
    "    match_count: int = 5,\n",
    "    doc_specific_type: str = None,\n",
    "    company_name: str = None,\n",
    "    doc_year_start: int = None,\n",
    "    doc_year_end: int = None,\n",
    "    doc_quarter: int = None\n",
    ") -> str:\n",
    "    print(f\"\\n--- Executing Tool: retrieve_financial_chunks ---\")\n",
    "    print(f\"  Query: '{query_text}'\")\n",
    "    print(f\"  User ID for retrieval: {user_id}\")\n",
    "    print(f\"  Filters: Type={doc_specific_type}, Company={company_name}, Year={doc_year_start}-{doc_year_end}, Qtr={doc_quarter}\")\n",
    "\n",
    "    # openai_client and supabase_service are accessed globally as per original script design\n",
    "    global openai_client, supabase_service\n",
    "    if openai_client is None or supabase_service is None:\n",
    "         return json.dumps({\"error\": \"Global clients not initialized. OpenAI or Supabase service is None.\"})\n",
    "\n",
    "    try:\n",
    "        query_embedding_list = openai_client.get_embeddings([query_text])\n",
    "        if not query_embedding_list:\n",
    "            print(f\"  Error: Failed to generate query embedding.\")\n",
    "            return json.dumps({\"error\": \"Failed to generate query embedding.\"})\n",
    "        query_embedding = query_embedding_list[0]\n",
    "        print(f\"  Query embedding generated.\")\n",
    "\n",
    "        print(f\"  Calling Supabase RPC 'match_chunks'...\")\n",
    "        response = supabase_service.client.rpc(\n",
    "            'match_chunks',\n",
    "            {\n",
    "                'query_embedding': query_embedding,\n",
    "                'match_count': match_count,\n",
    "                'user_id': user_id, # Pass the authenticated user_id to the RPC\n",
    "                'p_doc_specific_type': doc_specific_type,\n",
    "                'p_company_name': company_name,\n",
    "                'p_doc_year_start': doc_year_start,\n",
    "                'p_doc_year_end': doc_year_end,\n",
    "                'p_doc_quarter': doc_quarter\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "        if response.data is not None:\n",
    "            print(f\"  Retrieved {len(response.data)} chunks from Supabase.\")\n",
    "            processed_data = []\n",
    "            for chunk_dict in response.data:\n",
    "                 processed_chunk = {}\n",
    "                 for key, value in chunk_dict.items():\n",
    "                     if isinstance(value, uuid.UUID):\n",
    "                         processed_chunk[key] = str(value)\n",
    "                     else:\n",
    "                         processed_chunk[key] = value\n",
    "                 if 'document_filename' not in processed_chunk or processed_chunk['document_filename'] is None:\n",
    "                     print(f\"  Warning: RPC 'match_chunks' did not return 'document_filename' for a chunk.\")\n",
    "                     processed_chunk['document_filename'] = 'RPC_Missing_Doc_Name'\n",
    "                 if 'section_id' not in processed_chunk or processed_chunk['section_id'] is None:\n",
    "                     print(f\"  Warning: RPC 'match_chunks' did not return 'section_id' for a chunk.\")\n",
    "                     processed_chunk['section_id'] = str(processed_chunk.get('id', 'RPC_Missing_Section_ID'))\n",
    "                 if 'chunk_text' not in processed_chunk:\n",
    "                      processed_chunk['chunk_text'] = 'Chunk text missing from RPC.'\n",
    "                 if 'section_heading' not in processed_chunk:\n",
    "                      processed_chunk['section_heading'] = 'Section heading missing from RPC.'\n",
    "                 processed_data.append(processed_chunk)\n",
    "            result_json_string = json.dumps(processed_data, indent=2)\n",
    "            print(f\"  Returning JSON result ({len(result_json_string)} chars, first 500 for brevity):\\n{result_json_string[:500]}...\")\n",
    "            return result_json_string\n",
    "        elif hasattr(response, 'error') and response.error:\n",
    "             error_msg = f\"Supabase RPC 'match_chunks' error: {response.error.message if hasattr(response.error, 'message') else response.error}\"\n",
    "             print(f\"  Error: {error_msg}\")\n",
    "             return json.dumps({\"error\": error_msg})\n",
    "        else:\n",
    "            print(\"  Received unexpected response structure from Supabase RPC 'match_chunks'.\")\n",
    "            # ... (error details)\n",
    "            return json.dumps({\"error\": \"Unexpected response from Supabase RPC. Data and error fields were not accessible or were None when expected.\"})\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during chunk retrieval: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        return json.dumps({\"error\": f\"An unexpected error occurred during chunk retrieval: {str(e)}\"})\n",
    "\n",
    "# --- Define Function Declaration for Gemini ---\n",
    "retrieve_chunks_declaration = {\n",
    "    \"name\": \"retrieve_financial_chunks\",\n",
    "    \"description\": \"Searches and retrieves relevant text chunks from the user's uploaded financial documents based on their query and optional filters like company name, document type, year range, or quarter. Always use this tool to find information before answering questions about the user's financial documents.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query_text\": {\"type\": \"string\", \"description\": \"The user's original question or a refined search query.\"},\n",
    "            \"match_count\": {\"type\": \"integer\", \"description\": \"Max chunks to return. Default 5.\"},\n",
    "            \"doc_specific_type\": {\n",
    "                \"type\": \"string\", \n",
    "                \"description\": f\"Specific document type. Examples: {', '.join([item.value for item in FinancialDocSpecificType if item != FinancialDocSpecificType.UNKNOWN and item.value is not None])}.\",\n",
    "                \"enum\": [', '.join([item.value for item in FinancialDocSpecificType if item != FinancialDocSpecificType.UNKNOWN and item.value is not None])]\n",
    "                },\n",
    "            \"company_name\": {\"type\": \"string\", \"description\": \"Company name to filter by.\"},\n",
    "            \"doc_year_start\": {\"type\": \"integer\", \"description\": \"Starting fiscal year.\"},\n",
    "            \"doc_year_end\": {\"type\": \"integer\", \"description\": \"Ending fiscal year.\"},\n",
    "            \"doc_quarter\": {\n",
    "                \"type\": \"integer\", \n",
    "                \"description\": \"Fiscal quarter (1-4).\",\n",
    "                },\n",
    "        },\n",
    "        \"required\": [\"query_text\"]\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- SETUP FUNCTION ---\n",
    "def initialize_global_clients_and_authenticate():\n",
    "    \"\"\"\n",
    "    Loads .env, initializes global clients (OpenAI, Gemini, Supabase),\n",
    "    authenticates the test user, and sets global authenticated_user_id_str.\n",
    "    \"\"\"\n",
    "    global openai_client, gemini_client, supabase_service, authenticated_user_id_str, auth_client\n",
    "    load_dotenv()\n",
    "\n",
    "    TEST_EMAIL = os.environ.get(\"TEST_EMAIL\")\n",
    "    TEST_PASSWORD = os.environ.get(\"TEST_PASSWORD\")\n",
    "    if not TEST_EMAIL or not TEST_PASSWORD:\n",
    "        raise ValueError(\"TEST_EMAIL and TEST_PASSWORD must be set in your .env file.\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- Initializing clients and authenticating ---\")\n",
    "        openai_client = OpenAIClient()\n",
    "        gemini_client = GeminiClient() # This will be used by the main pipeline function\n",
    "\n",
    "        supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "        supabase_key = os.environ.get(\"SUPABASE_ANON_KEY\")\n",
    "        if not supabase_url or not supabase_key:\n",
    "            raise ValueError(\"SUPABASE_URL and SUPABASE_ANON_KEY must be set in your .env file.\")\n",
    "\n",
    "        auth_client = create_client(supabase_url, supabase_key)\n",
    "        print(\"Supabase client created.\")\n",
    "\n",
    "        print(f\"Attempting to sign in with email: {TEST_EMAIL}\")\n",
    "        auth_response = auth_client.auth.sign_in_with_password(\n",
    "            {\"email\": TEST_EMAIL, \"password\": TEST_PASSWORD}\n",
    "        )\n",
    "\n",
    "        if not auth_response or not auth_response.user:\n",
    "            error_detail = auth_response.error.message if hasattr(auth_response, 'error') and auth_response.error else \"Unknown authentication error\"\n",
    "            raise ConnectionError(f\"Supabase authentication failed: {error_detail}. Check credentials and Supabase Auth settings.\")\n",
    "\n",
    "        authenticated_user_id_str = str(auth_response.user.id)\n",
    "        print(f\"Authentication successful. User ID: {authenticated_user_id_str}\")\n",
    "\n",
    "        supabase_service = SupabaseService(supabase_client=auth_client)\n",
    "        print(\"Clients initialized and authenticated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Initialization or Authentication Error: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# --- MAIN QUERY PROCESSING PIPELINE FUNCTION ---\n",
    "def run_financial_query_pipeline(\n",
    "    user_query_text: str,\n",
    "    initial_conv_history: list,\n",
    "    gemini_client_instance: GeminiClient, # Pass the initialized Gemini client\n",
    "    auth_user_id: str, # Pass the authenticated user ID\n",
    "    tool_definition: types.Tool,\n",
    "    model_name_to_use: str\n",
    ") -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Manages the multi-turn conversation with Gemini to answer a financial query,\n",
    "    potentially using the retrieve_financial_chunks tool.\n",
    "    Assumes openai_client and supabase_service are available globally for retrieve_financial_chunks.\n",
    "    Returns the final answer text and the updated conversation history.\n",
    "    \"\"\"\n",
    "    current_conv_history = list(initial_conv_history) # Work on a mutable copy\n",
    "\n",
    "    try:\n",
    "        # 1. First call to LLM: Send user query and retrieval tool definition\n",
    "        print(f\"\\n--- Sending initial query to Gemini ({model_name_to_use}) ---\")\n",
    "        current_conv_history.append(types.Content(role=\"user\", parts=[types.Part(text=user_query_text)]))\n",
    "        print(f\"  Conversation History before first call:\\n{json.dumps(serialize_conversation_history(current_conv_history), indent=2)}\")\n",
    "\n",
    "        response = gemini_client_instance.client.models.generate_content(\n",
    "            model=model_name_to_use,\n",
    "            contents=current_conv_history,\n",
    "            config=types.GenerateContentConfig(\n",
    "                tools=[tool_definition],\n",
    "                automatic_function_calling= {\"disable\": True},\n",
    "                tool_config= {\"function_calling_config\": {\"mode\": \"any\"}},\n",
    "                temperature=0\n",
    "                )\n",
    "        )\n",
    "        print(f\"\\n--- Received response from first Gemini call ---\")\n",
    "\n",
    "        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:\n",
    "            error_msg = \"Error: Unexpected response structure or no candidates/parts from Gemini's first call.\"\n",
    "            print(error_msg)\n",
    "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback: print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "            return error_msg, current_conv_history\n",
    "\n",
    "        model_response_content = response.candidates[0].content\n",
    "        message_part = model_response_content.parts[0]\n",
    "        current_conv_history.append(model_response_content)\n",
    "        print(f\"\\n  Model's response content (first call) added to history.\")\n",
    "\n",
    "        # 2. Check if LLM requested a function call\n",
    "        if hasattr(message_part, 'function_call') and message_part.function_call:\n",
    "            function_call = message_part.function_call\n",
    "            print(f\"\\n--- Gemini requested function call: '{function_call.name}' ---\")\n",
    "            tool_args = dict(function_call.args)\n",
    "            print(f\"  Raw Arguments from LLM: {tool_args}\")\n",
    "\n",
    "            if function_call.name == \"retrieve_financial_chunks\":\n",
    "                print(\"  Recognized 'retrieve_financial_chunks' call.\")\n",
    "                tool_args['user_id'] = auth_user_id # Inject authenticated user_id\n",
    "                print(f\"  Tool_args *including* user_id for execution: {tool_args}\")\n",
    "\n",
    "                function_result_json = retrieve_financial_chunks(**tool_args)\n",
    "                print(f\"\\n--- Finished executing retrieve_financial_chunks ---\")\n",
    "                print(f\"  Function result (JSON string, first 500 chars):\\n{function_result_json[:500]}...\")\n",
    "\n",
    "                print(\"\\n--- Preparing enriched context and instructions for final Gemini call ---\")\n",
    "                try:\n",
    "                    function_response_data = json.loads(function_result_json)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding function result JSON: {function_result_json[:200]}...\")\n",
    "                    function_response_data = {\"error\": \"Invalid JSON from tool.\"}\n",
    "\n",
    "                function_response_part = types.Part.from_function_response(\n",
    "                    name=function_call.name,\n",
    "                    response={\"result\": function_response_data}\n",
    "                )\n",
    "                current_conv_history.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "                print(f\"  Raw function response part added to history.\")\n",
    "\n",
    "                formatted_snippets_text = format_chunks_for_llm(function_result_json)\n",
    "                final_instructions_text = create_final_answer_instructions(user_query_text, formatted_snippets_text)\n",
    "                current_conv_history.append(types.Content(role=\"user\", parts=[types.Part(text=final_instructions_text)]))\n",
    "                print(f\"  Formatted snippets and citation instructions added to history.\")\n",
    "                print(f\"\\n  Conversation History before second call (final answer generation):\\n{json.dumps(serialize_conversation_history(current_conv_history), indent=2)}\")\n",
    "\n",
    "                final_response = gemini_client_instance.client.models.generate_content(\n",
    "                    model=model_name_to_use,\n",
    "                    contents=current_conv_history\n",
    "                )\n",
    "                print(f\"\\n--- Received response from second Gemini call (Final Answer) ---\")\n",
    "\n",
    "                if final_response.candidates and final_response.candidates[0].content and final_response.candidates[0].content.parts:\n",
    "                    final_model_response_content = final_response.candidates[0].content\n",
    "                    current_conv_history.append(final_model_response_content)\n",
    "                    return final_response.text, current_conv_history\n",
    "                else:\n",
    "                    error_msg = \"Error: No final response text found after sending function result.\"\n",
    "                    print(error_msg)\n",
    "                    if hasattr(final_response, 'prompt_feedback') and final_response.prompt_feedback: print(f\"Final Response Prompt Feedback: {final_response.prompt_feedback}\")\n",
    "                    return error_msg, current_conv_history\n",
    "            else:\n",
    "                warning_msg = f\"Warning: LLM requested unknown function '{function_call.name}'. Stopping execution due to unknown function call.\"\n",
    "                print(warning_msg)\n",
    "                return warning_msg, current_conv_history\n",
    "        else:\n",
    "            print(\"\\n--- Gemini decided to answer directly (No Function Call Requested) ---\")\n",
    "            if hasattr(message_part, 'text') and message_part.text is not None:\n",
    "                print(message_part.text)\n",
    "                # current_conv_history already has model_response_content from first call\n",
    "                return message_part.text, current_conv_history\n",
    "            else:\n",
    "                no_text_msg = \"No text response found in the initial call and no function call made. Stopping execution after direct answer or unexpected initial response.\"\n",
    "                print(no_text_msg)\n",
    "                return no_text_msg, current_conv_history\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\\nAn unexpected error occurred during the Gemini interaction: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        return error_msg, current_conv_history\n",
    "\n",
    "\n",
    "import time\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initalize all variables\n",
    "    initialize_global_clients_and_authenticate() # Sets up global clients and user ID\n",
    "    \n",
    "    # User Query and Conversation History\n",
    "    t1 = time.time()\n",
    "    user_query_main = \"Whats the Gross Carrying Amount for Total intangible assets for tesla in 2021? Create a report of tesla for 2021 in markdown i can copy.\"\n",
    "    conversation_history_main = [] # Initialize fresh for each run, or load if continuing\n",
    "\n",
    "    print(f\"\\n--- User Query ---\")\n",
    "    print(user_query_main)\n",
    "\n",
    "    # Tool and Model Configuration\n",
    "    retrieval_tool_main = types.Tool(function_declarations=[retrieve_chunks_declaration])\n",
    "    # gemini_model_name_main = \"gemini-2.0-flash-lite\"\n",
    "    gemini_model_name_main = \"gemini-2.0-flash\"\n",
    "    # gemini_model_name_main = \"gemini-2.5-flash-preview-04-17\"\n",
    "\n",
    "    if gemini_client is None or authenticated_user_id_str is None:\n",
    "        print(\"Error: Global clients or user ID not initialized. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Run the main processing pipeline\n",
    "    final_answer_text, updated_history = run_financial_query_pipeline(\n",
    "        user_query_text=user_query_main,\n",
    "        initial_conv_history=conversation_history_main,\n",
    "        gemini_client_instance=gemini_client, # Use the globally initialized gemini_client\n",
    "        auth_user_id=authenticated_user_id_str, # Use the globally set user ID\n",
    "        tool_definition=retrieval_tool_main,\n",
    "        model_name_to_use=gemini_model_name_main\n",
    "    )\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Update conversation history if you plan to continue the conversation\n",
    "    conversation_history_main = updated_history\n",
    "\n",
    "    # Print the final answer\n",
    "    print_final_formatted_answer(final_answer_text)\n",
    "\n",
    "    # Optionally, print the full conversation history for debugging\n",
    "    print(\"\\n--- Full Conversation History (Serialized) ---\")\n",
    "    print(json.dumps(serialize_conversation_history(conversation_history_main), indent=2))\n",
    "    print(f\"\\n[TIMER] TOTAL ELAPSED: {(t2 - t1):.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
